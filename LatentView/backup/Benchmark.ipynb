{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "import logging\n",
    "import types\n",
    "from multiprocessing import Process, Pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the logging level \n",
    "# For info\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = #1e14\n",
    "high = 20 #201e16\n",
    "random.randint(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmarker(input_func: types.FunctionType) -> types.FunctionType:\n",
    "    \"\"\"Decorator to benchmark custom data structure implementation\n",
    "    Module to benchmark datastructure across large datasets with list comprehension and generator module\n",
    "    \"\"\"\n",
    "    DEFAULT_INFO_MESSAGE = 'The function {function_name} took {time_taken} seconds to complete'\n",
    "    def timer(*args, **kwargs) -> float:\n",
    "        \"\"\"Nested Timer function\n",
    "        Parameters\n",
    "        ----------\n",
    "            *args: Variable Length of non-keyworded argument list \n",
    "            **kwargs: Variable Length of non-keyworded argument dictionary  \n",
    "        -------\n",
    "        Tuple:\n",
    "            driver_name: Unique identifier key used in dynamic key-value store\n",
    "            distance: Processed distance (in miles) parsed from the input\n",
    "            time_spent: Processed journey time (in seconds) parsed from the input\n",
    "        \"\"\"\n",
    "        start_time = time.time()\n",
    "        _ = input_func(*args, **kwargs)\n",
    "        computation_time = time.time() - start_time\n",
    "#         logger.info(DEFAULT_INFO_MESSAGE.format(function_name= input_func.__name__, \n",
    "#                                                 time_taken= computation_time))\n",
    "        return computation_time\n",
    "    return timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmarker\n",
    "def evaluate_generator(n: int, expected: int) -> None:\n",
    "    \"\"\"Module to evaluate generator utility\n",
    "    \"\"\"\n",
    "    try:\n",
    "        true_value = sum(i for i in range(1, n+1))\n",
    "        assert true_value == expected\n",
    "    except AssertionError:\n",
    "        logger.error('Expected {expected} value mismatched with the obtained value {true_value}'.format(\n",
    "            expected=expected, true_value=true_value))\n",
    "        raise AssertionError('Terminating due to mismatch between expected and observed values')\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@benchmarker\n",
    "def evaluate_list_comprehension(n: int, expected: int) -> None:\n",
    "    \"\"\"Module to evaluate list comprehension utility\n",
    "    \"\"\"\n",
    "    try:\n",
    "        true_value = sum([i for i in range(1, n+1)])\n",
    "        assert true_value == expected\n",
    "    except AssertionError:\n",
    "        logger.error('Expected {expected} value mismatched with the obtained value {true_value}'.format(\n",
    "            expected=expected, true_value=true_value))\n",
    "        raise AssertionError('Terminating due to mismatch between expected and observed values')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_list_comprehension(5000000, 12500002500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10\n",
    "# Uniform Sampling\n",
    "low = 1e7\n",
    "high = 1e8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time = 0\n",
    "for _ in range(N):\n",
    "    value = random.randint(low, high)\n",
    "    expected = value * (value + 1) // 2\n",
    "    total_time += evaluate_generator(value, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time_generator = total_time_list_comprehension = 0\n",
    "for _ in range(2):\n",
    "    value = random.randint(low, high)\n",
    "    expected = value * (value + 1) // 2    \n",
    "    print (\"..1...\")\n",
    "    recv_end, send_end = Pipe(False)\n",
    "    p1 = Process(target = evaluate_generator(value, expected))\n",
    "    p2 = Process(target = evaluate_generator(value, expected))\n",
    "    p1.start()\n",
    "    p2.start()\n",
    "    p1.join()\n",
    "    p2.join()\n",
    "    \n",
    "    result_list = [x.recv() for x in pipe_list]\n",
    "    \n",
    "    \n",
    "    print (\"..2..\")\n",
    "    print (result_list)\n",
    "    total_time_generator = evaluate_generator(value, expected)\n",
    "    \n",
    "    total_time_list_comprehension += evaluate_list_comprehension(value, expected)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobs = []\n",
    "    pipe_list = []\n",
    "    for i in range(5):\n",
    "        recv_end, send_end = multiprocessing.Pipe(False)\n",
    "        p = multiprocessing.Process(target=worker, args=(i, send_end))\n",
    "        jobs.append(p)\n",
    "        pipe_list.append(recv_end)\n",
    "        p.start()\n",
    "\n",
    "    for proc in jobs:\n",
    "        proc.join()\n",
    "    result_list = [x.recv() for x in pipe_list]\n",
    "    print result_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.randint(low, high)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_list_comprehension(seed, expected, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BenchmarkFunction:\n",
    "    def __init__(self, operating_function, N=1000, low=1e14, high=1e16):\n",
    "        self.func = operating_function\n",
    "        self.N = N\n",
    "        self.low = low\n",
    "        self.high = high\n",
    "        \n",
    "    def evaluate_atomic(self):\n",
    "        value = random.randint(low, high)\n",
    "        expected = value * (value + 1) // 2\n",
    "        return self.func(value, expected)\n",
    "    \n",
    "    def evaluate(self):\n",
    "        computation_time = []\n",
    "        for _ in range(self.N):\n",
    "            computation_time += self.evaluate_atomic(),\n",
    "        return computation_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_eval = BenchmarkFunction(evaluate_generator)\n",
    "list_comprehension_eval = BenchmarkFunction(evaluate_list_comprehension)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "\n",
    "   config = configparser.RawConfigParser()\n",
    "   config.read('path_to_config.properties file')\n",
    "\n",
    "   details_dict = dict(config.items('SECTION_NAME'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = evaluate_generator(int(200), 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(evaluate_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "isinstance(evaluate_generator, types.FunctionType)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@timerfunc\n",
    "def long_runner():\n",
    "    for x in range(5):\n",
    "        sleep_time = random.choice(range(1,5))\n",
    "        time.sleep(sleep_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5\n",
    "sum(i for i in range(1, x+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(row for row in open(file_name))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
